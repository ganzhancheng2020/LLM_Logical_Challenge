{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: openai in /root/miniconda3/lib/python3.10/site-packages (1.17.1)\n",
      "Requirement already satisfied: tiktoken in /root/miniconda3/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: retry in /root/miniconda3/lib/python3.10/site-packages (0.9.2)\n",
      "Requirement already satisfied: dashscope in /root/miniconda3/lib/python3.10/site-packages (1.20.4)\n",
      "Requirement already satisfied: loguru in /root/miniconda3/lib/python3.10/site-packages (0.7.2)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /root/miniconda3/lib/python3.10/site-packages (from scipy) (1.26.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /root/miniconda3/lib/python3.10/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /root/miniconda3/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /root/miniconda3/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /root/miniconda3/lib/python3.10/site-packages (from openai) (2.7.3)\n",
      "Requirement already satisfied: sniffio in /root/miniconda3/lib/python3.10/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /root/miniconda3/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /root/miniconda3/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /root/miniconda3/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /root/miniconda3/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: decorator>=3.4.2 in /root/miniconda3/lib/python3.10/site-packages (from retry) (5.1.1)\n",
      "Requirement already satisfied: py<2.0.0,>=1.4.26 in /root/miniconda3/lib/python3.10/site-packages (from retry) (1.11.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from dashscope) (3.9.5)\n",
      "Requirement already satisfied: websocket-client in /root/miniconda3/lib/python3.10/site-packages (from dashscope) (1.7.0)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/miniconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /root/miniconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->dashscope) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->dashscope) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->dashscope) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->dashscope) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->dashscope) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->dashscope) (4.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy openai tiktoken retry dashscope loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import uuid\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "from scipy import sparse\n",
    "#from rank_bm25 import BM25Okapi\n",
    "#import jieba\n",
    "from http import HTTPStatus\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from loguru import logger\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523407886a8c4f7aba3e9b3aea2d60e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "mode_path = '../qwen/Qwen2-7B-Instruct/'\n",
    "lora_path = './output/Qwen2_instruct_lora/20240818/checkpoint-400' # 这里改称你的 lora 输出对应 checkpoint 地址\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\",torch_dtype=torch.float16, trust_remote_code=True).eval()\n",
    "\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "\n",
    "logger.remove()  # 移除默认的控制台输出\n",
    "logger.add(\"logs/app_{time:YYYY-MM-DD}.log\", level=\"INFO\", rotation=\"00:00\", retention=\"10 days\", compression=\"zip\")\n",
    "\n",
    "MODEL_NAME = 'Qwen2-7B-Instruct-lora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def api_retry(MODEL_NAME, query):\n",
    "    max_retries = 5\n",
    "    retry_delay = 60  # in seconds\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            return call_qwen_local(MODEL_NAME, query) #call_qwen_api(MODEL_NAME, query)\n",
    "        except Exception as e:\n",
    "            attempts += 1   \n",
    "            if attempts < max_retries:\n",
    "                logger.warning(f\"Attempt {attempts} failed for text: {query}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                logger.error(f\"All {max_retries} attempts failed for text: {query}. Error: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_qwen_local(MODEL_NAME, query):\n",
    "    inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": query}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "#tensor_gpu = tensor.to('cuda:0')\n",
    "    gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_qwen_api(MODEL_NAME, query):\n",
    "    # 这里采用dashscope的api调用模型推理，通过http传输的json封装返回结果\n",
    "    \n",
    "    client = OpenAI(\n",
    "        base_url=\"http://localhost:8000/v1\",\n",
    "        api_key=\"sk-xxx\", # 随便填写，只是为了通过接口参数校验\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "      model=MODEL_NAME,\n",
    "      messages=[\n",
    "                # {'role':'system','content':'你是一个解决推理任务的专家，你需要分析出问题中的每个实体以及响应关系。然后根据问题一步步推理出结果。并且给出正确的结论。'},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "      ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 这里定义了prompt推理模版\n",
    "\n",
    "def get_prompt(problem, question, options):\n",
    "\n",
    "    options = '\\n'.join(f\"{'ABCDEFG'[i]}. {o}\" for i, o in enumerate(options))\n",
    "\n",
    "    prompt = f\"\"\"你是一个逻辑推理专家，擅长解决逻辑推理问题。以下是一个逻辑推理的题目，形式为单项选择题。所有的问题都是（close-world assumption）闭世界假设，即未观测事实都为假。请逐步分析问题并在最后一行输出答案，最后一行的格式为\"答案是：A\"。题目如下：\n",
    "\n",
    "### 题目:\n",
    "{problem}\n",
    "### 问题:\n",
    "{question}\n",
    "### 选项:\n",
    "{options}\n",
    "\"\"\"\n",
    "    # print(prompt)\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 这里使用extract抽取模获得抽取的结果\n",
    "\n",
    "def extract(input_text):\n",
    "    ans_pattern = re.compile(r\"(.)\", re.S)\n",
    "\n",
    "    # ans_pattern = re.compile(r\"答案是：(.)\", re.S)\n",
    "    problems = ans_pattern.findall(input_text)\n",
    "    # print(problems)\n",
    "    if(problems == []):\n",
    "        return 'A'\n",
    "    return problems[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def most_frequent_char(char1, char2, char3):\n",
    "    # 创建一个字典来存储每个字符的出现次数\n",
    "    frequency = {char1: 0, char2: 0, char3: 0}\n",
    "    \n",
    "    # 增加每个字符的出现次数\n",
    "    frequency[char1] += 1\n",
    "    frequency[char2] += 1\n",
    "    frequency[char3] += 1\n",
    "    \n",
    "    # 找到出现次数最多的字符\n",
    "    most_frequent = max(frequency, key=frequency.get)\n",
    "    \n",
    "    return most_frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_datas(datas,MODEL_NAME):\n",
    "    results = []\n",
    "\n",
    "    # 送入多线程任务\n",
    "    for data in tqdm(datas, desc=\"Submitting tasks\", total=len(datas)):\n",
    "        problem = data['problem']\n",
    "        for id,question in enumerate(data['questions']):\n",
    "            prompt = get_prompt(problem, \n",
    "                                question['question'], \n",
    "                                question['options'],\n",
    "                                    )\n",
    "            res,res1,res2 = api_retry(MODEL_NAME, prompt),api_retry(MODEL_NAME, prompt),api_retry(MODEL_NAME, prompt)\n",
    "            extract_response,extract_response1,extract_response2 = extract(res),extract(res1),extract(res2)\n",
    "            ans = most_frequent_char(extract_response,extract_response1,extract_response2)\n",
    "            data['questions'][id]['answer'] = ans\n",
    "            results.append(data) \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(ifn, ofn):\n",
    "    if os.path.exists(ofn):\n",
    "        pass\n",
    "    data = []\n",
    "    # 按行读取数据\n",
    "    with open(ifn) as reader:\n",
    "        for line in reader:\n",
    "            sample = json.loads(line)\n",
    "            data.append(sample)\n",
    "    datas = data\n",
    "    # print(data)\n",
    "    # 均匀地分成多个数据集\n",
    "    return_list = process_datas(datas,MODEL_NAME)\n",
    "    print(len(return_list))\n",
    "    print(\"All tasks finished!\")\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(ofn):\n",
    "    data = []\n",
    "    with open(ofn) as reader:\n",
    "        for line in reader:\n",
    "            sample = json.loads(line)\n",
    "            data.append(sample)\n",
    "\n",
    "    pse = 0\n",
    "    cnt = 0\n",
    "    tot = 0\n",
    "    for task in data:\n",
    "        for question in task['questions']:\n",
    "            \n",
    "            if MODEL_NAME in question:\n",
    "                tot += 1\n",
    "                cnt += question[MODEL_NAME] == question['answer']\n",
    "            else:\n",
    "                pse += 1\n",
    "\n",
    "    print(cnt, tot, cnt/tot, pse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Submitting tasks:  87%|████████▋ | 437/500 [5:20:23<36:37, 34.88s/it]    "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    return_list = main('data/round1_test_data.jsonl', 'upload.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def has_complete_answer(questions):\n",
    "    # 这里假设完整答案的判断逻辑是：每个question都有一个'answer'键\n",
    "    for question in questions:\n",
    "        if 'answer' not in question:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def filter_problems(data):\n",
    "    result = []\n",
    "    problem_set = set()\n",
    "\n",
    "    for item in data:\n",
    "        # print('处理的item' ,item)\n",
    "        problem = item['problem']\n",
    "        if problem in problem_set:\n",
    "            # 找到已存在的字典\n",
    "            for existing_item in result:\n",
    "                if existing_item['problem'] == problem:\n",
    "                    # 如果当前字典有完整答案，替换已存在的字典\n",
    "                    if has_complete_answer(item['questions']):\n",
    "                        existing_item['questions'] = item['questions']\n",
    "                        existing_item['id'] = item['id']\n",
    "                    break\n",
    "        else:\n",
    "            # 如果当前字典有完整答案，添加到结果列表\n",
    "            if has_complete_answer(item['questions']):\n",
    "                result.append(item)\n",
    "                problem_set.add(problem)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "return_list\n",
    "return_list = filter_problems(return_list)\n",
    "sorted_data = sorted(return_list, key=lambda x: int(str(x['id'])[-3:]))\n",
    "print(sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_missing_ids(dict_list):\n",
    "    # 提取所有序号\n",
    "    extracted_ids = {int(d['id'][-3:]) for d in dict_list}\n",
    "    \n",
    "    # 创建0-500的序号集合\n",
    "    all_ids = set(range(500))\n",
    "    \n",
    "    # 找出缺失的序号\n",
    "    missing_ids = all_ids - extracted_ids\n",
    "    \n",
    "    return sorted(missing_ids)\n",
    "\n",
    "# 示例字典列表\n",
    "dict_list = sorted_data\n",
    "\n",
    "# 找出缺失的序号\n",
    "missing_ids = find_missing_ids(dict_list)\n",
    "print(\"缺失的序号:\", missing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(missing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data  = []\n",
    "with open('round1_test_data.jsonl') as reader:\n",
    "    for id,line in enumerate(reader):\n",
    "        if(id in missing_ids):\n",
    "            sample = json.loads(line)\n",
    "            for question in sample['questions']:\n",
    "                question['answer'] = 'A'\n",
    "            sorted_data.append(sample)\n",
    "sorted_data = sorted(sorted_data, key=lambda x: int(str(x['id'])[-3:]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('upload.jsonl', 'w') as writer:\n",
    "    for sample in sorted_data:\n",
    "        writer.write(json.dumps(sample, ensure_ascii=False))\n",
    "        writer.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
